{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7006018",
   "metadata": {},
   "source": [
    "---\n",
    "- Leaf Classification (Smooth vs Serrated) using PyTorch + ConvNeXt\n",
    "- Author: Messaoudi-Belkacem\n",
    "- Date: 2025-11-11\n",
    "---\n",
    "\n",
    "# 1. IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713c3641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, math, time\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.models import convnext_tiny, ConvNeXt_Tiny_Weights\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    balanced_accuracy_score, cohen_kappa_score,\n",
    "    log_loss, roc_auc_score, classification_report,\n",
    "    confusion_matrix, RocCurveDisplay, PrecisionRecallDisplay,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Configure device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                      (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "print(f\"ðŸ–¥ï¸  Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ðŸŽ® GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"ðŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb2f961",
   "metadata": {},
   "source": [
    "# 2. DATA CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f654f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/kaggle/input/plant-leaves-dataset/feuilles_plantes\"\n",
    "\n",
    "weights = ConvNeXt_Tiny_Weights.IMAGENET1K_V1\n",
    "default_transforms = weights.transforms()\n",
    "\n",
    "# Training transformations\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.3),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=default_transforms.mean, std=default_transforms.std),\n",
    "    transforms.RandomErasing(p=0.2)\n",
    "])\n",
    "\n",
    "# Validation/Test transformations (no augmentation)\n",
    "transform_eval = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=default_transforms.mean,\n",
    "        std=default_transforms.std\n",
    "    )\n",
    "])\n",
    "\n",
    "# Load full dataset\n",
    "full_dataset = datasets.ImageFolder(DATA_DIR, transform=transform_train)\n",
    "num_classes = len(full_dataset.classes)\n",
    "\n",
    "print(f\"\\nðŸ“ Dataset Info:\")\n",
    "print(f\"   Classes: {full_dataset.classes}\")\n",
    "print(f\"   Total images: {len(full_dataset)}\")\n",
    "print(f\"   Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754a12ee",
   "metadata": {},
   "source": [
    "# 3. STRATIFIED TRAIN/VAL/TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d39f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Splits:\n",
    "    train: Subset\n",
    "    val: Subset\n",
    "    test: Subset\n",
    "\n",
    "def stratified_split(dataset: datasets.ImageFolder,\n",
    "                     train_ratio: float = 0.7,\n",
    "                     val_ratio: float = 0.15,\n",
    "                     test_ratio: float = 0.15,\n",
    "                     seed: int = 0) -> Splits:\n",
    "    \"\"\"Returns train/val/test Subsets with stratified class distribution.\"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    \n",
    "    # Group indices by class label\n",
    "    class_indices = {}\n",
    "    for idx, (_, label) in enumerate(dataset.imgs):\n",
    "        if label not in class_indices:\n",
    "            class_indices[label] = []\n",
    "        class_indices[label].append(idx)\n",
    "    \n",
    "    train_indices = []\n",
    "    val_indices = []\n",
    "    test_indices = []\n",
    "    \n",
    "    # Split each class proportionally\n",
    "    for label, indices in class_indices.items():\n",
    "        indices = np.array(indices)\n",
    "        rng.shuffle(indices)\n",
    "        \n",
    "        n_total = len(indices)\n",
    "        n_train = int(n_total * train_ratio)\n",
    "        n_val = int(n_total * val_ratio)\n",
    "        \n",
    "        train_indices.extend(indices[:n_train].tolist())\n",
    "        val_indices.extend(indices[n_train:n_train+n_val].tolist())\n",
    "        test_indices.extend(indices[n_train+n_val:].tolist())\n",
    "    \n",
    "    return Splits(\n",
    "        train=Subset(dataset, train_indices),\n",
    "        val=Subset(dataset, val_indices),\n",
    "        test=Subset(dataset, test_indices)\n",
    "    )\n",
    "\n",
    "# Create splits\n",
    "splits = stratified_split(full_dataset, 0.7, 0.15, 0.15, seed=0)\n",
    "\n",
    "# Create optimized DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    splits.train, \n",
    "    batch_size=64,\n",
    "    shuffle=True, \n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "full_dataset.transform = transform_eval\n",
    "val_loader = DataLoader(\n",
    "    splits.val, \n",
    "    batch_size=256,  # Increased from 64\n",
    "    shuffle=False, \n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    splits.test, \n",
    "    batch_size=256,\n",
    "    shuffle=False, \n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ“Š Split sizes:\")\n",
    "print(f\"   Train: {len(splits.train)}\")\n",
    "print(f\"   Val: {len(splits.val)}\")\n",
    "print(f\"   Test: {len(splits.test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19344ccb",
   "metadata": {},
   "source": [
    "# 4. MODEL ARCHITECTURE (ConvNeXt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6644da0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_convnext_model(num_classes=2, model_size='tiny'):\n",
    "    \"\"\"Create ConvNeXt model with pretrained weights.\"\"\"\n",
    "    print(f\"\\nðŸ—ï¸  Building ConvNeXt-{model_size} model...\")\n",
    "    \n",
    "    if model_size == 'tiny':\n",
    "        model = models.convnext_tiny(weights='IMAGENET1K_V1')\n",
    "    elif model_size == 'small':\n",
    "        model = models.convnext_small(weights='IMAGENET1K_V1')\n",
    "    elif model_size == 'base':\n",
    "        model = models.convnext_base(weights='IMAGENET1K_V1')\n",
    "    else:\n",
    "        model = models.convnext_large(weights='IMAGENET1K_V1')\n",
    "    \n",
    "    # Replace classifier head\n",
    "    in_features = model.classifier[2].in_features\n",
    "    model.classifier[2] = nn.Linear(in_features, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_convnext_model(num_classes=num_classes, model_size='tiny')\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f8718d",
   "metadata": {},
   "source": [
    "# 5. TRAINING SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a612f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    ")\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c59835",
   "metadata": {},
   "source": [
    "# 6. TRAINING & EVALUATION FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45497f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, scaler):\n",
    "    \"\"\"Train for one epoch with mixed precision.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training', leave=False)\n",
    "    \n",
    "    for imgs, labels in pbar:\n",
    "        imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.3f}',\n",
    "            'acc': f'{100.*correct/total:.1f}%'\n",
    "        })\n",
    "    \n",
    "    return total_loss / len(loader.dataset), 100. * correct / total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion):\n",
    "    \"\"\"Evaluate model without gradient computation.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    y_true, y_pred, y_proba = [], [], []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Validation', bar_format='{l_bar}{bar:30}{r_bar}', leave=False)\n",
    "    \n",
    "    for imgs, labels in pbar:\n",
    "        imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "        \n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "        y_proba.extend(probs[:, 1].cpu().numpy())\n",
    "        \n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.3f}',\n",
    "            'acc': f'{100.*correct/total:.1f}%'\n",
    "        })\n",
    "    \n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    accuracy = 100. * correct / total\n",
    "    return avg_loss, np.array(y_true), np.array(y_pred), np.array(y_proba), accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a7e942",
   "metadata": {},
   "source": [
    "# 7. TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc1b7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸš€ Starting training...\\n\")\n",
    "best_val_acc = 0.0\n",
    "epochs = 30\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, criterion, scaler)\n",
    "    \n",
    "    # Validation\n",
    "    va_loss, y_t, y_p, y_s, va_acc = evaluate(model, val_loader, criterion)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(va_loss)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"ðŸ“ˆ Epoch {epoch+1:02d}/{epochs} | \"\n",
    "          f\"Train: {tr_loss:.4f} ({tr_acc:.2f}%) | \"\n",
    "          f\"Val: {va_loss:.4f} ({va_acc:.2f}%) | \"\n",
    "          f\"LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if va_acc > best_val_acc:\n",
    "        best_val_acc = va_acc\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(f\"   âœ… New best model saved! (Val Acc: {best_val_acc:.2f}%)\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(f\"\\nâœ¨ Training complete! Best Val Accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c16a85",
   "metadata": {},
   "source": [
    "# 8. FINAL EVALUATION ON TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f37483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ§ª Evaluating on test set...\")\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "te_loss, y_true, y_pred, y_proba, te_acc = evaluate(model, test_loader, criterion)\n",
    "\n",
    "# Identify positive class\n",
    "classes = full_dataset.classes\n",
    "pos_class_name = classes[0]\n",
    "pos_idx = classes.index(pos_class_name)\n",
    "\n",
    "# Compute comprehensive metrics\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "    y_true, y_pred, average=\"binary\", pos_label=pos_idx\n",
    ")\n",
    "bacc = balanced_accuracy_score(y_true, y_pred)\n",
    "kappa = cohen_kappa_score(y_true, y_pred)\n",
    "ll = log_loss(y_true, np.column_stack([1-y_proba, y_proba]))\n",
    "auc = roc_auc_score((y_true==pos_idx).astype(int), y_proba)\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ¯ TEST SET PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Test Loss         : {te_loss:.4f}\")\n",
    "print(f\"Accuracy          : {acc:.4f} ({acc*100:.2f}%)\")\n",
    "print(f\"Precision         : {prec:.4f}\")\n",
    "print(f\"Recall            : {rec:.4f}\")\n",
    "print(f\"F1 Score          : {f1:.4f}\")\n",
    "print(f\"Balanced Accuracy : {bacc:.4f}\")\n",
    "print(f\"Cohen's Kappa     : {kappa:.4f}\")\n",
    "print(f\"Log Loss          : {ll:.4f}\")\n",
    "print(f\"AUC-ROC           : {auc:.4f}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“‹ CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_true, y_pred, target_names=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b4f038",
   "metadata": {},
   "source": [
    "# 9. VISUALIZATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc1d3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ“Š Generating visualizations...\")\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "ConfusionMatrixDisplay(cm, display_labels=classes).plot(values_format='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix (Test Set)\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "RocCurveDisplay.from_predictions((y_true==pos_idx).astype(int), y_proba)\n",
    "plt.title(\"ROC Curve (Test Set)\", fontsize=14, fontweight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "PrecisionRecallDisplay.from_predictions((y_true==pos_idx).astype(int), y_proba)\n",
    "plt.title(\"Precision-Recall Curve (Test Set)\", fontsize=14, fontweight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('precision_recall_curve.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… All done! Model saved as 'best_model.pth'\")\n",
    "print(f\"ðŸ“ˆ Final Test Accuracy: {acc*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
